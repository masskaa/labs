{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c858ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/10 13:45:52 WARN Utils: Your hostname, student-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/12/10 13:45:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/10 13:45:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/10 13:45:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/10 13:45:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/12/10 13:45:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/10 13:46:13 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:191)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
      "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:314)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:245)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:732)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Ulysses, by James Joyce'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lab4\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "textFiles = sc.textFile('hdfs://localhost:9000//user/student/gutenberg/*')\n",
    "\n",
    "textFiles.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b6b9f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3637, 'which'),\n",
       " (1691, 'their'),\n",
       " (1224, 'there'),\n",
       " (954, 'other'),\n",
       " (884, 'would'),\n",
       " (846, 'these')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textFiles\\\n",
    "    .flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "\n",
    "\n",
    "counts = words\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .filter(lambda word: len(word[0]) >= 5)\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .map(lambda word: (word[1],word[0]))\\\n",
    "    .sortByKey(False)\n",
    "\n",
    "counts.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "066d4e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10929, ('', '')),\n",
       " (6900, ('of', 'the')),\n",
       " (3501, ('in', 'the')),\n",
       " (2208, ('to', 'the')),\n",
       " (1637, ('and', 'the')),\n",
       " (1618, ('on', 'the')),\n",
       " (1231, ('from', 'the'))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = textFiles.flatMap(lambda line: line.split(\".\"))\\\n",
    "    .map(lambda line: line.strip().split(\" \"))\\\n",
    "    .flatMap(lambda words: (tuple(word) for word in zip(words, words[1: ])))\n",
    "\n",
    "pair_counts = bigrams\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .map(lambda word: (word[1],word[0]))\\\n",
    "    .sortByKey(False)\n",
    "\n",
    "pair_counts.take(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "136d089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Nationalgymnasiummuseumsanatoriumandsuspensoriumsordinaryprivatdocentge']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams1 = textFiles.flatMap(lambda line: line.split(\"-\"))\\\n",
    "        .flatMap(lambda line: line.split(\" \"))\\\n",
    "        .flatMap(lambda line: line.split(\"â€”\"))\\\n",
    "\n",
    "\n",
    "counts1 = words\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .map(lambda word: (word[0]))\\\n",
    "    .sortBy(lambda word: len(word) * -1)\n",
    "\n",
    "counts1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "973ec6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('A', 4943), ('B', 2990), ('C', 2675), ('D', 1843), ('E', 1405), ('F', 2148)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams2  = textFiles\\\n",
    "    .filter(lambda word: len(word) > 0 and word[0].isalpha())\\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "counts2 = bigrams2.keys()\\\n",
    "    .map(lambda word: (word[0].upper(), 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\\\n",
    "    .sortByKey()\n",
    "\n",
    "counts3.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b471ad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2022, 320)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 241:=====================================================> (62 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2022, 320)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "weatherLines = sc.textFile('hdfs://localhost:9000/user/student/weather/*')\n",
    "weatherColumns = [\"year\",\"temperature\", \"quality\"]\n",
    "weatherDF = weatherLines\\\n",
    "    .map(lambda val: (int(val[15:19]), int(val[87:92]), int(val[92:93])))\\\n",
    "    .toDF(weatherColumns)\n",
    "\n",
    "weatherDF.createOrReplaceTempView(\"Weather\")\n",
    "\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "    SELECT year, max(temperature) FROM Weather\n",
    "    WHERE quality IN (0,1,4,5,9) AND temperature != 9999\n",
    "    GROUP BY year\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = weatherDF\\\n",
    "    .where(weatherDF.quality.isin([0, 1, 4, 5, 9]) & (weatherDF.temperature != 9999))\\\n",
    "    .groupBy(weatherDF.year)\\\n",
    "    .agg(max(weatherDF.temperature))\n",
    "\n",
    "print(sqlWay.rdd.map(lambda x: (x[0], x[1])).collect())\n",
    "print(dataFrameWay.rdd.map(lambda x: (x[0], x[1])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd81a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce820c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
